<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
	<title>Blog posts</title>
	<link href="https://stuglaser.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
    <link href="https://stuglaser.github.io/blog/"/>
	<updated>2024-04-30T00:00:00+00:00</updated>
	<id>https://stuglaser.github.io/blog/feed.xml</id>
	<entry xml:lang="en">
		<title>Why robotics needs determinism</title>
		<published>2024-04-30T00:00:00+00:00</published>
		<updated>2024-04-30T00:00:00+00:00</updated>
		<link href="https://stuglaser.github.io/blog/why-determinism/" type="text/html"/>
		<id>https://stuglaser.github.io/blog/why-determinism/</id>
		<content type="html">&lt;h1 id=&quot;determinism-in-robotics-software&quot;&gt;Determinism in robotics software&lt;a class=&quot;zola-anchor&quot; href=&quot;#determinism-in-robotics-software&quot; aria-label=&quot;Anchor link for: determinism-in-robotics-software&quot; style=&quot;visibility: hidden;&quot;&gt;#&lt;&#x2F;a&gt;&lt;&#x2F;h1&gt;
&lt;p&gt;It’s a lot harder to debug something when it gives different results each time you run it. Robotics software is almost predetermined to be a non-reproducible nightmare. There is multithreading, lots of floating point math, and unusual hardware architectures, all of which harm the reproducibility of code. In a typical robotics project, it’s very common to run the same code on the same data and get very different results.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;types-of-reproducibility-goals&quot;&gt;Types of reproducibility goals&lt;a class=&quot;zola-anchor&quot; href=&quot;#types-of-reproducibility-goals&quot; aria-label=&quot;Anchor link for: types-of-reproducibility-goals&quot; style=&quot;visibility: hidden;&quot;&gt;#&lt;&#x2F;a&gt;&lt;&#x2F;h1&gt;
&lt;p&gt;There are two possible goals of reproducibility:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Onboard reproducibility: Reproducing a situation that happened on your robot precisely on a developer machine.&lt;&#x2F;li&gt;
&lt;li&gt;Dev reproducibility: Running the same software on a developer’s machine produces the same results every time, but doesn’t necessarily reproduce the onboard results.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;Dev reproducibility is very important for having a sane development process. Without dev reproducibility, every test is flakey. Debugging problems is extremely difficult, because adding log statements to code and rerunning it will give a different result. It’s even difficult to try out new ideas in a given component, since upstream components may produce varying outputs.&lt;&#x2F;p&gt;
&lt;p&gt;Onboard reproducibility is nice too, but it is very difficult to achieve. The onboard software gets run as quickly as possible, so messages can get dropped and execution of components can be reordered. To get onboard-to-offboard reproducibility, all message passing events must be memorized and replayed. Even worse, if a message is received by an onboard component, but not saved to disk, it cannot be replayed and reproducibility of this example is impossible. Differences in CPU or GPU can cause differences in floating point computations. Getting onboard code to reproduce perfectly offboard is very very difficult.&lt;&#x2F;p&gt;
&lt;p&gt;But onboard reproducibility may not be a necessary goal. Some code has probably already changed from when the robot ran to when the debugging is happening, so reproducing the onboard event precisely tells you nothing about what the current code (which hopefully has some fixes and improvements) would do. Instead, the events that matter are the ones that still fail on the current code, and debugging those only needs dev reproducibility.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;why-does-dev-non-determinism-happen&quot;&gt;Why does dev non-determinism happen?&lt;a class=&quot;zola-anchor&quot; href=&quot;#why-does-dev-non-determinism-happen&quot; aria-label=&quot;Anchor link for: why-does-dev-non-determinism-happen&quot; style=&quot;visibility: hidden;&quot;&gt;#&lt;&#x2F;a&gt;&lt;&#x2F;h1&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;stuglaser.github.io&#x2F;blog&#x2F;why-determinism&#x2F;component_graph.png&quot; alt=&quot;component-graph&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Typically, robotics software is made of a bunch of nodes that pass messages to each other, mostly sending messages in one direction from upstream sensor components, to perception components, to planner components, and then to controllers. Each component receives some messages, runs, and sends out messages to its downstream consumers. The main cause of nondeterminism is that the first time we replay a situation, a component receives a certain set of messages, but the next replay it receives a different set. For example, at timestep T=90, node C may receive (A_90, B_90), but on the second replay it may receive (A_90, B_91), or even (A_90, &lt;em&gt;dropped&lt;&#x2F;em&gt;).&lt;&#x2F;p&gt;
&lt;p&gt;Onboard, this kind of nondeterminism is expected. Messages can arrive slightly early or slightly late, and since components should not wait too long to execute, on some time steps they may miss certain messages.&lt;&#x2F;p&gt;
&lt;p&gt;It is also possible to have nondeterminism within a given component. Typical issues include not seeding a random number generator, being overly clever with multithreading inside the component, using data structures that are non-deterministic themselves, and using wall time (which we’ll discuss later). Non determinism within a component is a lot easier to track down than issues from message passing, since it’s self-contained and can be debugged without involving the rest of the system.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;how-to-get-reproducibility&quot;&gt;How to get reproducibility?&lt;a class=&quot;zola-anchor&quot; href=&quot;#how-to-get-reproducibility&quot; aria-label=&quot;Anchor link for: how-to-get-reproducibility&quot; style=&quot;visibility: hidden;&quot;&gt;#&lt;&#x2F;a&gt;&lt;&#x2F;h1&gt;
&lt;p&gt;To get reproducibility, your robotics framework needs, for each component, to control the order of messages presented to that component. As long as the order of presented messages is deterministic (and the component is internally deterministic), then your overall simulation should be deterministic.&lt;&#x2F;p&gt;
&lt;p&gt;The least error-prone way I can think of to get determinism is that each component has its own independent clock, which behaves similar to the logical clock concept in distributed systems. Every time the framework passes a message, it also passes sim-time along with the message. Downstream components do not immediately process received messages. Instead they have an input queue of messages, sorted by sim-time. A component can only progress forwards to time T once all it’s upstream components have passed it a message at or beyond time T. Once the component can reach time T, it can pop all messages at or before T from its input queues and process them.&lt;&#x2F;p&gt;
&lt;p&gt;Some other details that add complexity are:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;If component A runs and doesn’t send a message directly to its downstream component B, it should still pass along a clock signal so that B knows its clock can move forwards in time.&lt;&#x2F;li&gt;
&lt;li&gt;If you have a cycle in the component graph, you need some way to prevent the cycle from deadlocking.&lt;&#x2F;li&gt;
&lt;li&gt;You need some type of backpressure to prevent upstream components from flooding the message queues of downstream components.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h1 id=&quot;what-software-to-use&quot;&gt;What software to use&lt;a class=&quot;zola-anchor&quot; href=&quot;#what-software-to-use&quot; aria-label=&quot;Anchor link for: what-software-to-use&quot; style=&quot;visibility: hidden;&quot;&gt;#&lt;&#x2F;a&gt;&lt;&#x2F;h1&gt;
&lt;p&gt;Unfortunately, I don’t know of any open source frameworks that provide determinism. ROS does not have this out of the box. Its message passing system is designed to run on the robot itself, so it delivers messages as quickly as possible and doesn’t try to achieve determinism. I have heard second-hand about companies having internal frameworks for achieving determinism in their simulations. Unfortunately none of these frameworks have been open sourced that I know of.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Sensors and time synchronization</title>
		<published>2024-04-13T00:00:00+00:00</published>
		<updated>2024-04-13T00:00:00+00:00</updated>
		<link href="https://stuglaser.github.io/blog/sensor-time/" type="text/html"/>
		<id>https://stuglaser.github.io/blog/sensor-time/</id>
		<content type="html">&lt;p&gt;Finally you’ve hooked up all the software for your robot, and it’s time for it to take its first steps. You turn it all on and nothing works right. You start debugging and visualize the perceived world state, and everything looks blurry. &lt;&#x2F;p&gt;
&lt;p&gt;Handling time properly in robotics is an underappreciated task. Until things start going wrong and it becomes fully appreciated. Robots need to fuse together information from multiple sensors, build a consistent view of the world, and take action based on that view. But getting a clear understanding of the scene requires that the times associated with sensor data be precise.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;synchronized-sensor-clocks&quot;&gt;Synchronized sensor clocks&lt;a class=&quot;zola-anchor&quot; href=&quot;#synchronized-sensor-clocks&quot; aria-label=&quot;Anchor link for: synchronized-sensor-clocks&quot; style=&quot;visibility: hidden;&quot;&gt;#&lt;&#x2F;a&gt;&lt;&#x2F;h1&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;stuglaser.github.io&#x2F;blog&#x2F;sensor-time&#x2F;fusion_architecture.png&quot; alt=&quot;fusion-architecture&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Most robotic systems pull together data from multiple sensors, combine them into a single view of the world, and then plan an action. To combine the sensor data together, we need to be able to relate each item to each other, in time and in space. Spatially, we calibrate the sensor poses so we can relate their data, but we also need to figure out how to relate the data in time. We need to label every item of sensor data with the precise time that it was acquired, so that the measurements and detections between different sensors are consistent.&lt;&#x2F;p&gt;
&lt;p&gt;How precise do our time labels need to be? If your robot and the world is stationary, getting time measurements wrong doesn’t matter. Nothing has moved, so an image at any point would look the same. However, once anything starts moving, the time precision on our sensor measurements matters.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;stuglaser.github.io&#x2F;blog&#x2F;sensor-time&#x2F;sensed_object_moving.png&quot; alt=&quot;sensed-object-moving&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;For example, consider a Clearpath Warthog UGV, which can move at 11 mph (18 km&#x2F;hr). For a sensor pointed sideways, every loss of 1 ms precision in time maps to 5 cm in positional error. This can be a lot, especially if you are trying to estimate velocities of other objects by considering their positions over time. But rotational velocities lead to even more imprecision than translational velocities. Even small rotations can generate a lot of distortion.&lt;&#x2F;p&gt;
&lt;p&gt;Maybe this doesn’t sound like a big amount of error. However, consider that you’ll have to fight imprecision throughout your system. Sensor fusion will give poor results, and root causing it will show that the sensor observations being fused together do not line up as you’d expect them too. There are several possible causes of the observations not lining up: Positional calibration, sensor error, depth estimation error, and code errors all lead to imprecise measurements, and you’ll have to hunt down each of these while debugging your system. Leaving time imprecision in your system makes it much harder to recognize which of the other sources are causing measurement fusion errors, so getting time synchronization correct will save you a lot of time later on.&lt;&#x2F;p&gt;
&lt;p&gt;How do you get time-synchronized sensor measurements? It does depend on the sensor, but in general, most (good) robotics sensors accept a hardware signal. The Hesai OT128 lidar takes Precision Time Protocol (PTP) inputs over its ethernet port, synchronizes its internal clock to the PTP master clock, and then timestamps packets of lidar sensor data with the acquisition time. The Sony IMX490, a common automotive camera, has both a PTP input and a hardware trigger input, where an electrical signal triggers the camera shutter.&lt;&#x2F;p&gt;
&lt;p&gt;Many sensors, especially cameras, also have a software trigger. I do not recommend using software triggers without another type of time synchronization. The path from your camera software sending a trigger command, through the operating system’s drivers, through network, USB, or CAN, and then finally to the camera gives a lot of room for time imprecision to slip in. There’s an unknown amount of time between commanding a software trigger and the sensor acquiring an image.&lt;&#x2F;p&gt;
&lt;p&gt;IMU’s and mechanical joint sensors need especially tight time synchronization. If you are solving a difficult control problem, such as high-speed aerial maneuvering or bipedal running, then your control algorithms may be sensitive to even sub-millisecond errors in time accuracy.&lt;&#x2F;p&gt;
&lt;p&gt;Many issues with time synchronization can be debugged and adjusted for later in development. However, incorrect sensor time stamping cannot be fixed later. It’s important to get this right as early as possible.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;sensor-distortion&quot;&gt;Sensor distortion&lt;a class=&quot;zola-anchor&quot; href=&quot;#sensor-distortion&quot; aria-label=&quot;Anchor link for: sensor-distortion&quot; style=&quot;visibility: hidden;&quot;&gt;#&lt;&#x2F;a&gt;&lt;&#x2F;h1&gt;
&lt;p&gt;In addition to synchronizing between sensors, sometimes we must adjust for time within the sensor data itself. Many lidars and cameras do not capture the entire spin or image at a single instant, but take readings over a period of time. Depending on the speed of movement of your robotic platform, you may need to adjust for time within a single packet of sensor data.&lt;&#x2F;p&gt;
&lt;p&gt;Cameras are either global shutter or rolling shutter. A global shutter camera captures all pixels simultaneously, while a rolling shutter typically captures one row of pixels at a time, working its way from the top to the bottom of the image. Clearly global shutter cameras are better for time synchronization. However, global shutter cameras tend to be more expensive, and often lack other features, such as HDR capturing or higher resolution. Since rolling shutter cameras can sometimes be a better choice overall, you may need to compensate for the rolling shutter. If the camera and world are relatively static, rolling shutter adjustment might be unnecessary, but in higher-speed situations it can make a big difference. If the camera will be rotated at high speeds, the smearing from the rolling shutter can be significant, or if you are operating in dim lighting, the increase in shutter time will also make the rolling shutter smearing more significant.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;stuglaser.github.io&#x2F;blog&#x2F;sensor-time&#x2F;rolling_shutter_shear.png&quot; alt=&quot;rolling-shutter-shear&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Similarly, spinning lidars only see part of the environment at a time. A typical spinning lidar operates at 10Hz, so the first and last observations of the spin are roughly 100ms apart. A fast moving robot or faster objects in the environment can move enough to distort the lidar spin.&lt;&#x2F;p&gt;
&lt;p&gt;Both rolling shutter and spinning lidars can be adjusted for the motion of the robot platform. The odometry (typically from a Kalman filter pulling together wheel odometry and any other odom sources) and joint positions of the robot provide the sensor positions over time. You can then solve for the location of the sensor given the time each pixel or lidar point is captured, and either undistort the sensor data directly or apply undistortion to the later processing of the sensor data (detections, velocity estimates, etc…).&lt;&#x2F;p&gt;
&lt;p&gt;It’s more difficult to apply undistortion to objects moving quickly in the scene. You don’t know the velocity of these objects a priori, so you cannot solve for the undistortion directly. Methods for undistorting external objects are difficult and highly specific.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;final-thoughts&quot;&gt;Final thoughts&lt;a class=&quot;zola-anchor&quot; href=&quot;#final-thoughts&quot; aria-label=&quot;Anchor link for: final-thoughts&quot; style=&quot;visibility: hidden;&quot;&gt;#&lt;&#x2F;a&gt;&lt;&#x2F;h1&gt;
&lt;p&gt;It&#x27;s nearly impossible to correct poor time synchronization after the fact, so it&#x27;s a really good idea to timestamp your sensors properly at the design-phase of a project.&lt;&#x2F;p&gt;
</content>
	</entry>
</feed>