<!doctype html><html lang=en><head><meta charset=utf-8><meta content="width=device-width,initial-scale=1.0" name=viewport><meta content="light dark" name=color-scheme><title>Sensors and time synchronization</title><link href=/img/favicon-32x32.png rel=icon sizes=32x32 type=image/png><link href=/img/favicon-16x16.png rel=icon sizes=16x16 type=image/png><link href=/img/apple-touch-icon.png rel=apple-touch-icon sizes=180x180><style>body{--primary-color:#5871a2;--primary-pale-color:#5871a210;--text-color:#3c4043;--text-pale-color:#94969f;--bg-color:#fff;--highlight-mark-color:#5f75b035;--callout-note-color:#5871a2;--callout-important-color:#8062b0;--callout-warning-color:#936e51;--callout-alert-color:#bc5252;--callout-question-color:#477389;--callout-tip-color:#3c8460}body.dark{--primary-color:#5d77ac;--primary-pale-color:#5d77ac20;--text-color:#9197a5;--text-pale-color:#747983;--bg-color:#202124;--highlight-mark-color:#5f75b035;--callout-note-color:#5d77ac;--callout-important-color:#8062b0;--callout-warning-color:#936e51;--callout-alert-color:#bc5252;--callout-question-color:#477389;--callout-tip-color:#3c8460}body{--main-font:ui-sans-serif,system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";--code-font:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;--homepage-max-width:768px;--main-max-width:768px;--avatar-size:60px;--icon-size:20px;--homepage-font-size:16px;--homepage-line-height:1.75;--paragraph-font-size:16px;--paragraph-line-height:1.75;--aside-font-size:15px;--img-border-radius:0;--callout-border-radius:0;--detail-border-radius:0;--dark-mode-img-brightness:.75;--dark-mode-chart-brightness:.75;--inline-code-border-radius:2px;--inline-code-bg-color:var(--primary-pale-color);--block-code-border-radius:0;--block-code-border-color:var(--primary-color);--detail-border-color:var(--primary-color)}</style><link href=/main.css rel=stylesheet><link href=/hl-light.css id=hl rel=stylesheet><body class=post><script>const theme=sessionStorage.getItem('theme');const match=window.matchMedia("(prefers-color-scheme: dark)").matches;if(theme&&theme=='dark'||!theme&&match){document.body.classList.add('dark');const a=document.querySelector('link#hl');if(a)a.href='/hl-dark.css'}</script><header class=blur><div id=header-wrapper><nav><a class=instant href=/>stu</a><span class=separator>::</span><a class=instant href=/blog>blog</a></nav><div id=btns><a aria-label="rss feed" href=/blog/feed.xml id=rss-btn><svg viewbox="0 0 24 24" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M3 17C5.20914 17 7 18.7909 7 21H3V17ZM3 10C9.07513 10 14 14.9249 14 21H12C12 16.0294 7.97056 12 3 12V10ZM3 3C12.9411 3 21 11.0589 21 21H19C19 12.1634 11.8366 5 3 5V3Z" fill=currentColor></path></svg></a><button aria-label="theme switch" data-moon-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="20" height="20"><path d="M10 7C10 10.866 13.134 14 17 14C18.9584 14 20.729 13.1957 21.9995 11.8995C22 11.933 22 11.9665 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12C2 6.47715 6.47715 2 12 2C12.0335 2 12.067 2 12.1005 2.00049C10.8043 3.27098 10 5.04157 10 7ZM4 12C4 16.4183 7.58172 20 12 20C15.0583 20 17.7158 18.2839 19.062 15.7621C18.3945 15.9187 17.7035 16 17 16C12.0294 16 8 11.9706 8 7C8 6.29648 8.08133 5.60547 8.2379 4.938C5.71611 6.28423 4 8.9417 4 12Z" fill="currentColor"></path></svg>' data-sun-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="20" height="20"><path d="M12 18C8.68629 18 6 15.3137 6 12C6 8.68629 8.68629 6 12 6C15.3137 6 18 8.68629 18 12C18 15.3137 15.3137 18 12 18ZM12 16C14.2091 16 16 14.2091 16 12C16 9.79086 14.2091 8 12 8C9.79086 8 8 9.79086 8 12C8 14.2091 9.79086 16 12 16ZM11 1H13V4H11V1ZM11 20H13V23H11V20ZM3.51472 4.92893L4.92893 3.51472L7.05025 5.63604L5.63604 7.05025L3.51472 4.92893ZM16.9497 18.364L18.364 16.9497L20.4853 19.0711L19.0711 20.4853L16.9497 18.364ZM19.0711 3.51472L20.4853 4.92893L18.364 7.05025L16.9497 5.63604L19.0711 3.51472ZM5.63604 16.9497L7.05025 18.364L4.92893 20.4853L3.51472 19.0711L5.63604 16.9497ZM23 11V13H20V11H23ZM4 11V13H1V11H4Z" fill="currentColor"></path></svg>' id=theme-toggle><svg viewbox="0 0 24 24" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M10 7C10 10.866 13.134 14 17 14C18.9584 14 20.729 13.1957 21.9995 11.8995C22 11.933 22 11.9665 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12C2 6.47715 6.47715 2 12 2C12.0335 2 12.067 2 12.1005 2.00049C10.8043 3.27098 10 5.04157 10 7ZM4 12C4 16.4183 7.58172 20 12 20C15.0583 20 17.7158 18.2839 19.062 15.7621C18.3945 15.9187 17.7035 16 17 16C12.0294 16 8 11.9706 8 7C8 6.29648 8.08133 5.60547 8.2379 4.938C5.71611 6.28423 4 8.9417 4 12Z" fill=currentColor></path></svg></button><button aria-label="table of content" id=toc-toggle><svg viewbox="0 0 24 24" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M3 4H21V6H3V4ZM3 11H15V13H3V11ZM3 18H21V20H3V18Z" fill=currentColor></path></svg></button></div></div></header><dialog id=rss-mask><div><a href=https://stuglaser.github.io/blog/feed.xml>https://stuglaser.github.io/blog/feed.xml</a><button data-check-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="20" height="20"><path d="M10.0007 15.1709L19.1931 5.97852L20.6073 7.39273L10.0007 17.9993L3.63672 11.6354L5.05093 10.2212L10.0007 15.1709Z" fill="currentColor"></path></svg>' data-copy-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="20" height="20"><path d="M6.9998 6V3C6.9998 2.44772 7.44752 2 7.9998 2H19.9998C20.5521 2 20.9998 2.44772 20.9998 3V17C20.9998 17.5523 20.5521 18 19.9998 18H16.9998V20.9991C16.9998 21.5519 16.5499 22 15.993 22H4.00666C3.45059 22 3 21.5554 3 20.9991L3.0026 7.00087C3.0027 6.44811 3.45264 6 4.00942 6H6.9998ZM5.00242 8L5.00019 20H14.9998V8H5.00242ZM8.9998 6H16.9998V16H18.9998V4H8.9998V6Z" fill="currentColor"></path></svg>' aria-label=copy autofocus data-link=https://stuglaser.github.io/blog/feed.xml><svg viewbox="0 0 24 24" height=20 width=20 xmlns=http://www.w3.org/2000/svg><path d="M6.9998 6V3C6.9998 2.44772 7.44752 2 7.9998 2H19.9998C20.5521 2 20.9998 2.44772 20.9998 3V17C20.9998 17.5523 20.5521 18 19.9998 18H16.9998V20.9991C16.9998 21.5519 16.5499 22 15.993 22H4.00666C3.45059 22 3 21.5554 3 20.9991L3.0026 7.00087C3.0027 6.44811 3.45264 6 4.00942 6H6.9998ZM5.00242 8L5.00019 20H14.9998V8H5.00242ZM8.9998 6H16.9998V16H18.9998V4H8.9998V6Z" fill=currentColor></path></svg></button></div></dialog><div id=wrapper><div id=blank></div><aside><nav><ul><li><a class=h2 href=#synchronized-sensor-clocks>Synchronized sensor clocks</a><li><a class=h2 href=#sensor-distortion>Sensor distortion</a><li><a class=h2 href=#final-thoughts>Final thoughts</a></ul></nav></aside><main><div><div data-check-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="20" height="20"><path d="M10.0007 15.1709L19.1931 5.97852L20.6073 7.39273L10.0007 17.9993L3.63672 11.6354L5.05093 10.2212L10.0007 15.1709Z" fill="currentColor"></path></svg>' data-copy-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="20" height="20"><path d="M6.9998 6V3C6.9998 2.44772 7.44752 2 7.9998 2H19.9998C20.5521 2 20.9998 2.44772 20.9998 3V17C20.9998 17.5523 20.5521 18 19.9998 18H16.9998V20.9991C16.9998 21.5519 16.5499 22 15.993 22H4.00666C3.45059 22 3 21.5554 3 20.9991L3.0026 7.00087C3.0027 6.44811 3.45264 6 4.00942 6H6.9998ZM5.00242 8L5.00019 20H14.9998V8H5.00242ZM8.9998 6H16.9998V16H18.9998V4H8.9998V6Z" fill="currentColor"></path></svg>' id=copy-cfg style=display:none></div><article data-backlink-icon='<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="20" height="20"><path d="M9.41421 8L18.0208 16.6066L16.6066 18.0208L8 9.41421V17H6V6H17V8H9.41421Z" fill="currentColor"></path></svg>' class=prose><h1>Sensors and time synchronization</h1><div id=post-info><div id=date><span id=publish>2024-04-13</span></div><div id=tags><a class=instant href=https://stuglaser.github.io/tags/time><span>#</span>time</a></div></div><p>Finally you’ve hooked up all the software for your robot, and it’s time for it to take its first steps. You turn it all on and nothing works right. You start debugging and visualize the perceived world state, and everything looks blurry.<p>Handling time properly in robotics is an underappreciated task. Until things start going wrong and it becomes fully appreciated. Robots need to fuse together information from multiple sensors, build a consistent view of the world, and take action based on that view. But getting a clear understanding of the scene requires that the times associated with sensor data be precise.<h1 id=synchronized-sensor-clocks>Synchronized sensor clocks<a aria-label="Anchor link for: synchronized-sensor-clocks" class=zola-anchor href=#synchronized-sensor-clocks style=visibility:hidden>#</a></h1><p><img alt=fusion-architecture src=https://stuglaser.github.io/blog/sensor-time/fusion_architecture.png><p>Most robotic systems pull together data from multiple sensors, combine them into a single view of the world, and then plan an action. To combine the sensor data together, we need to be able to relate each item to each other, in time and in space. Spatially, we calibrate the sensor poses so we can relate their data, but we also need to figure out how to relate the data in time. We need to label every item of sensor data with the precise time that it was acquired, so that the measurements and detections between different sensors are consistent.<p>How precise do our time labels need to be? If your robot and the world is stationary, getting time measurements wrong doesn’t matter. Nothing has moved, so an image at any point would look the same. However, once anything starts moving, the time precision on our sensor measurements matters.<p><img alt=sensed-object-moving src=https://stuglaser.github.io/blog/sensor-time/sensed_object_moving.png><p>For example, consider a Clearpath Warthog UGV, which can move at 11 mph (18 km/hr). For a sensor pointed sideways, every loss of 1 ms precision in time maps to 5 cm in positional error. This can be a lot, especially if you are trying to estimate velocities of other objects by considering their positions over time. But rotational velocities lead to even more imprecision than translational velocities. Even small rotations can generate a lot of distortion.<p>Maybe this doesn’t sound like a big amount of error. However, consider that you’ll have to fight imprecision throughout your system. Sensor fusion will give poor results, and root causing it will show that the sensor observations being fused together do not line up as you’d expect them too. There are several possible causes of the observations not lining up: Positional calibration, sensor error, depth estimation error, and code errors all lead to imprecise measurements, and you’ll have to hunt down each of these while debugging your system. Leaving time imprecision in your system makes it much harder to recognize which of the other sources are causing measurement fusion errors, so getting time synchronization correct will save you a lot of time later on.<p>How do you get time-synchronized sensor measurements? It does depend on the sensor, but in general, most (good) robotics sensors accept a hardware signal. The Hesai OT128 lidar takes Precision Time Protocol (PTP) inputs over its ethernet port, synchronizes its internal clock to the PTP master clock, and then timestamps packets of lidar sensor data with the acquisition time. The Sony IMX490, a common automotive camera, has both a PTP input and a hardware trigger input, where an electrical signal triggers the camera shutter.<p>Many sensors, especially cameras, also have a software trigger. I do not recommend using software triggers without another type of time synchronization. The path from your camera software sending a trigger command, through the operating system’s drivers, through network, USB, or CAN, and then finally to the camera gives a lot of room for time imprecision to slip in. There’s an unknown amount of time between commanding a software trigger and the sensor acquiring an image.<p>IMU’s and mechanical joint sensors need especially tight time synchronization. If you are solving a difficult control problem, such as high-speed aerial maneuvering or bipedal running, then your control algorithms may be sensitive to even sub-millisecond errors in time accuracy.<p>Many issues with time synchronization can be debugged and adjusted for later in development. However, incorrect sensor time stamping cannot be fixed later. It’s important to get this right as early as possible.<h1 id=sensor-distortion>Sensor distortion<a aria-label="Anchor link for: sensor-distortion" class=zola-anchor href=#sensor-distortion style=visibility:hidden>#</a></h1><p>In addition to synchronizing between sensors, sometimes we must adjust for time within the sensor data itself. Many lidars and cameras do not capture the entire spin or image at a single instant, but take readings over a period of time. Depending on the speed of movement of your robotic platform, you may need to adjust for time within a single packet of sensor data.<p>Cameras are either global shutter or rolling shutter. A global shutter camera captures all pixels simultaneously, while a rolling shutter typically captures one row of pixels at a time, working its way from the top to the bottom of the image. Clearly global shutter cameras are better for time synchronization. However, global shutter cameras tend to be more expensive, and often lack other features, such as HDR capturing or higher resolution. Since rolling shutter cameras can sometimes be a better choice overall, you may need to compensate for the rolling shutter. If the camera and world are relatively static, rolling shutter adjustment might be unnecessary, but in higher-speed situations it can make a big difference. If the camera will be rotated at high speeds, the smearing from the rolling shutter can be significant, or if you are operating in dim lighting, the increase in shutter time will also make the rolling shutter smearing more significant.<p><img alt=rolling-shutter-shear src=https://stuglaser.github.io/blog/sensor-time/rolling_shutter_shear.png><p>Similarly, spinning lidars only see part of the environment at a time. A typical spinning lidar operates at 10Hz, so the first and last observations of the spin are roughly 100ms apart. A fast moving robot or faster objects in the environment can move enough to distort the lidar spin.<p>Both rolling shutter and spinning lidars can be adjusted for the motion of the robot platform. The odometry (typically from a Kalman filter pulling together wheel odometry and any other odom sources) and joint positions of the robot provide the sensor positions over time. You can then solve for the location of the sensor given the time each pixel or lidar point is captured, and either undistort the sensor data directly or apply undistortion to the later processing of the sensor data (detections, velocity estimates, etc…).<p>It’s more difficult to apply undistortion to objects moving quickly in the scene. You don’t know the velocity of these objects a priori, so you cannot solve for the undistortion directly. Methods for undistorting external objects are difficult and highly specific.<h1 id=final-thoughts>Final thoughts<a aria-label="Anchor link for: final-thoughts" class=zola-anchor href=#final-thoughts style=visibility:hidden>#</a></h1><p>It's nearly impossible to correct poor time synchronization after the fact, so it's a really good idea to timestamp your sensors properly at the design-phase of a project.</article></div><footer><div class=copyright><p>© 2024 Stu</div></footer></main></div><script src=/js/lightense.min.js></script><script src=/js/main.js></script>